<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformerの構造③ (大学生・教員向け) - デコーダ</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body { margin: 0; padding: 0; font-family: 'M PLUS Rounded 1c', sans-serif; background-color: #f0f9ff; overflow: auto; }
        .slide { width: 1280px; min-height: 720px; margin: 0 auto; position: relative; overflow: auto; background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); /* Green Gradient */ box-shadow: 0 10px 20px rgba(0,0,0,0.1); border-radius: 12px; padding: 40px; display: flex; flex-direction: column; }
        .decoder-block { border: 3px solid #a855f7; /* Purple-600 */ border-radius: 10px; padding: 20px; background: rgba(243, 232, 255, 0.7); /* Purple-100 with transparency */ width: 90%; margin: 0 auto; }
        .component-box { background: white; border: 1px solid #d8b4fe; /* Purple-300 */ border-radius: 6px; padding: 8px 10px; margin-bottom: 10px; /* Margin調整 */ text-align: center; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
        .arrow { font-size: 1.3rem; /* サイズ調整 */ color: #c084fc; /* Purple-400 */ margin: 3px 0; }
        .highlight { background-color: #a7f3d0; padding: 2px 5px; border-radius: 3px; font-weight: bold; }
        .encoder-output { border: 2px dashed #3b82f6; padding: 5px; border-radius: 4px; background-color: #eff6ff; color: #3b82f6; font-size: 0.8rem; font-weight: bold; display: inline-block; margin-bottom: 10px; }
    </style>
</head>
<body>
    <div class="slide">
        <div class="flex justify-between items-center mb-4">
            <h1 class="text-4xl font-bold text-green-800">Transformerの構造③：デコーダ<br>〜出力シーケンスの生成〜</h1>
            <i class="fas fa-cog text-purple-600 text-5xl"></i>
        </div>

        <div class="flex-grow flex flex-col items-center justify-center">
            <p class="text-xl text-center text-gray-700 mb-4">
                デコーダは、エンコーダからの文脈情報と、自身が生成した過去のトークンに基づいて、次のトークンを予測し、出力シーケンスを生成します。
            </p>

            <div class="decoder-block">
                <p class="text-center font-bold text-purple-700 mb-2">デコーダの内部構造 (例: 「Cats」の次の単語を予測)</p>

                <div class="text-center mb-2">
                    <span class="encoder-output">エンコーダからの文脈ベクトル</span>
                    <p class="font-semibold text-sm">直前の出力: 「Cats」</p>
                </div>
                <div class="flex justify-center"><i class="fas fa-arrow-down arrow"></i></div>

                <div class="component-box max-w-md mx-auto">
                    <p class="font-bold text-purple-700">① 出力埋め込み (Output Embedding) と マスク付き位置エンコーディング (Masked Positional Encoding)</p>
                    <p class="text-sm mt-1">直前に生成されたトークン（例: 「Cats」）をベクトル表現に変換し、位置情報を加えます。自己注意において未来の情報を参照しないようにマスク処理を行います。</p>
                </div>
                <div class="flex justify-center"><i class="fas fa-arrow-down arrow"></i></div>

                <div class="component-box max-w-md mx-auto">
                    <p class="font-bold text-purple-700">② マスク付きマルチヘッド自己注意機構 (Masked Multi-Head Self-Attention)</p>
                    <p class="text-sm mt-1">デコーダが生成した過去のトークン間の関連性を計算します。未来のトークンの情報はマスクされ、予測に利用できません。</p>
                </div>
                <div class="flex justify-center"><i class="fas fa-arrow-down arrow"></i></div>

                <div class="component-box max-w-md mx-auto">
                    <p class="font-bold text-purple-700">③ マルチヘッド・エンコーダ-デコーダ注意機構 (Multi-Head Encoder-Decoder Attention)</p>
                    <p class="text-sm mt-1">エンコーダからの文脈ベクトルと、デコーダの自己注意層からの出力を組み合わせて、入力シーケンスと出力シーケンスの関連性を学習します。これにより、翻訳元と翻訳先の対応付けを行います。</p>
                    <p class="text-xs text-gray-500 mt-1">← <span class="encoder-output">エンコーダからの文脈ベクトル</span> を利用</p>
                </div>
                <div class="flex justify-center"><i class="fas fa-arrow-down arrow"></i></div>

                <div class="component-box max-w-md mx-auto">
                    <p class="font-bold text-purple-700">④ 順伝播ネットワーク (Feed-Forward Network)</p>
                    <p class="text-sm mt-1">注意機構からの出力をさらに処理し、非線形変換を適用して表現力を高めます。</p>
                </div>
                <div class="flex justify-center"><i class="fas fa-arrow-down arrow"></i></div>

                <div class="component-box max-w-md mx-auto">
                    <p class="font-bold text-purple-700">⑤ 線形層 (Linear Layer) と Softmax関数</p>
                    <p class="text-sm mt-1">最終的な出力として、語彙数に対応した次元を持つベクトルを生成し、Softmax関数によって各単語の生成確率を計算します。</p>
                    <p class="text-xs text-gray-500">例: 「are」の確率が最も高い → 次の単語として「are」を出力</p>
                </div>

                <p class="text-center text-sm font-bold text-purple-800 mt-2">
                    このデコーダの処理を繰り返し、終端記号が出力されるまで、または指定された最大長に達するまで、トークンが生成されます。
                </p>
            </div>

            <div class="mt-4 bg-green-50 p-4 rounded-lg max-w-3xl text-center">
                <p class="font-bold text-green-800"><i class="fas fa-info-circle mr-1"></i>ポイント</p>
                <p class="text-sm text-gray-700">デコーダは、<span class="font-bold">エンコーダからの文脈情報</span>と、<span class="font-bold">マスクされた自己注意</span>によって過去の生成履歴を参照しながら、<span class="font-bold">エンコーダ-デコーダ注意</span>を通じて入力との関連性を学習し、次の適切なトークンを予測します。</p>
            </div>
        </div>

        <div class="absolute bottom-4 right-8 bg-white bg-opacity-70 rounded-full px-4 py-1">
            <span class="text-gray-600">15 / 48</span>
        </div>
    </div>
</body>
</html>