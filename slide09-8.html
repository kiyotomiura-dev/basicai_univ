<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformerの要素技術 (大学生・教員向け) - 埋め込み (Embedding)</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.0.0/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=M+PLUS+Rounded+1c:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body { margin: 0; padding: 0; font-family: 'M PLUS Rounded 1c', sans-serif; background-color: #f0f9ff; overflow: auto; }
        .slide { width: 1280px; min-height: 720px; margin: 0 auto; position: relative; overflow: auto; background: linear-gradient(135deg, #f0fdf4 0%, #dcfce7 100%); /* Green Gradient */ box-shadow: 0 10px 20px rgba(0,0,0,0.1); border-radius: 12px; padding: 40px; display: flex; flex-direction: column; }
        .vector-space { border: 2px dashed #6ee7b7; /* Emerald-300 */ border-radius: 10px; padding: 20px; margin-top: 20px; background-color: rgba(255, 255, 255, 0.8); position: relative; height: 300px; /* 高さを確保 */ }
        .word-point { position: absolute; background-color: #10b981; /* Emerald-500 */ color: white; padding: 4px 8px; border-radius: 15px; font-size: 0.9rem; font-weight: bold; box-shadow: 0 2px 5px rgba(0,0,0,0.2); }
        .word-point.animal { background-color: #fb923c; } /* Orange for animals */
        .word-point.vehicle { background-color: #60a5fa; } /* Blue for vehicles */
        .arrow-dist { position: absolute; content: ''; width: 0; height: 0; border-top: 8px solid transparent; border-bottom: 8px solid transparent; }
        .highlight { background-color: #a7f3d0; padding: 2px 5px; border-radius: 3px; font-weight: bold; }
        .code-example { font-family: monospace; background-color: #e5e7eb; padding: 8px; border-radius: 4px; font-size: 0.9rem; overflow-x: auto; /* 横スクロール */ white-space: nowrap; /* 折り返さない */ }
    </style>
</head>
<body>
    <div class="slide">
        <div class="flex justify-between items-center mb-6">
            <h1 class="text-4xl font-bold text-green-800">Transformerの要素技術：埋め込み (Embedding)</h1>
            <i class="fas fa-code text-green-600 text-5xl"></i>
        </div>

       <div class="flex-grow flex flex-col items-center justify-center">
            <p class="text-xl text-center text-gray-700 mb-4">
                Transformerモデルでは、自然言語の単語やトークンを、連続的なベクトル空間内のベクトルに変換する<span class="highlight text-green-900">埋め込み (Embedding)</span> という技術が用いられます。これにより、離散的な言語情報を数値的な表現として扱うことが可能になります。
            </p>

            <div class="bg-white p-6 rounded-lg shadow-md max-w-4xl w-full mb-6">
                <h2 class="text-2xl font-bold text-center text-green-700 mb-4">埋め込み (Embedding) の概念</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6 items-center">
                    <div>
                        <p class="mb-3"><i class="fas fa-check text-green-500 mr-2"></i>各<span class="font-bold">トークン</span>（単語、サブワードなど）を、その<span class="font-bold">意味的および文脈的特徴</span>を捉えた実数値ベクトルに変換します。</p>
                        <p class="mb-3"><i class="fas fa-check text-green-500 mr-2"></i>このベクトル空間において、意味的に類似したトークンは互いに近い位置に配置される傾向があります。</p>
                        <p class="mb-3"><i class="fas fa-check text-green-500 mr-2"></i>埋め込みベクトルの次元数は、通常数百から数千であり、より多くの次元を持つほど、より複雑な意味的関係性を表現できます。</p>
                        <p class="text-sm text-gray-600 mb-2">例：「猫」というトークンの埋め込みベクトル（概念的な表現）</p>
                        <div class="code-example">
                            猫 = [0.12, -0.55, 2.31, 0.08, ..., -1.04] <span class="text-xs text-gray-500"> (多数の要素からなるベクトル)</span>
                        </div>
                    </div>
                    <div class="vector-space">
                        <h3 class="text-center font-semibold text-gray-700 mb-2">意味空間の視覚化 (低次元での近似)</h3>
                        <div class="word-point animal" style="top: 20%; left: 25%;">猫</div>
                        <div class="word-point animal" style="top: 35%; left: 15%;">犬</div>
                        <div class="word-point animal" style="top: 50%; left: 30%;">ペット</div>
                        <div class="word-point vehicle" style="top: 60%; left: 70%;">車</div>
                        <div class="word-point vehicle" style="top: 75%; left: 60%;">バス</div>
                        <div class="word-point vehicle" style="top: 70%; left: 85%;">飛行機</div>
                        <div class="word-point" style="top: 20%; left: 70%;">リンゴ</div>

                        <div class="absolute top-1/2 left-1/2 transform -translate-x-1/2 -translate-y-1/2 text-center">
                            <p class="text-sm text-red-600 font-bold bg-white p-1 rounded shadow-lg">
                                <i class="fas fa-map-marker-alt mr-1"></i>意味的に近いトークンは<br>ベクトル空間内で近くに配置
                            </p>
                            <svg class="absolute top-[-60px] left-[-40px] w-[100px] h-[50px] opacity-70" viewbox="0 0 100 50">
                                <line x1="10" y1="40" x2="90" y2="10" stroke="#fb923c" stroke-width="2" stroke-dasharray="4"/>
                            </svg>
                            <p class="text-xs text-gray-500 absolute top-[50px] left-0 w-full">（ベクトル間の距離は意味の類似度を示す）</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="grid grid-cols-1 md:grid-cols-2 gap-6 max-w-4xl w-full">
                <div class="bg-white p-4 rounded-lg shadow-md">
                    <h3 class="font-bold text-green-700 mb-2"><i class="fas fa-cogs mr-1"></i>埋め込みの学習</h3>
                    <p class="text-sm text-gray-700">埋め込みベクトルは、大規模なテキストコーパスを用いて、周囲の単語（文脈）に基づいてトークンの意味を予測するなどのタスクを通じて学習されます。Transformerモデル自体も、学習の過程で入力と出力の埋め込みを学習します。</p>
                </div>
                <div class="bg-white p-4 rounded-lg shadow-md">
                    <h3 class="font-bold text-green-700 mb-2"><i class="fas fa-thumbs-up mr-1"></i>埋め込みの利点</h3>
                    <p class="text-sm text-gray-700">埋め込みを用いることで、Transformerモデルは以下のことが可能になります。</p>
                    <ul class="list-disc list-inside text-sm text-gray-700 ml-2 mt-1">
                        <li>トークン間の<span class="font-bold">意味的類似性</span>を定量的に評価できます。</li>
                        <li>「王様」と「男性」、「女王様」と「女性」のような<span class="font-bold">アナロジー関係</span>を捉えることができます。</li>
                        <li>高次元の言語情報を、Transformerの各層で処理しやすい低次元のベクトル表現に変換できます。</li>
                    </ul>
                </div>
            </div>

            <div class="mt-6 bg-green-50 p-4 rounded-lg max-w-3xl text-center">
                <p class="font-bold text-green-800"><i class="fas fa-info-circle mr-1"></i>ポイント</p>
                <p class="text-sm text-gray-700"><span class="font-bold">埋め込み (Embedding)</span> は、Transformerモデルが自然言語を理解し、処理するための<span class="font-bold">基礎となる重要な技術</span>です。これにより、言葉の持つ意味的な情報を数値化し、モデルが学習できる形に変換します。</p>
            </div>
        </div>

        <div class="absolute bottom-4 right-8 bg-white bg-opacity-70 rounded-full px-4 py-1">
            <span class="text-gray-600">17 / 48</span>
        </div>
    </div>
</body>
</html>